
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/multi_task.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorials_multi_task.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_multi_task.py:


Task-specific policy in multi-task environments
================================================
This tutorial details how multi-task policies and batched environments can be used.

.. GENERATED FROM PYTHON SOURCE LINES 8-11

At the end of this tutorial, you will be capable of writing policies that
can compute actions in diverse settings using a distinct set of weights.
You will also be able to execute diverse environments in parallel.

.. GENERATED FROM PYTHON SOURCE LINES 11-17

.. code-block:: Python



    import torch
    from tensordict.nn import TensorDictModule, TensorDictSequential
    from torch import nn








.. GENERATED FROM PYTHON SOURCE LINES 39-44

.. code-block:: Python


    from torchrl.envs import CatTensors, Compose, DoubleToFloat, ParallelEnv, TransformedEnv
    from torchrl.envs.libs.dm_control import DMControlEnv
    from torchrl.modules import MLP








.. GENERATED FROM PYTHON SOURCE LINES 45-47

We design two environments, one humanoid that must complete the stand task
and another that must learn to walk.

.. GENERATED FROM PYTHON SOURCE LINES 47-75

.. code-block:: Python


    env1 = DMControlEnv("humanoid", "stand")
    env1_obs_keys = list(env1.observation_spec.keys())
    env1 = TransformedEnv(
        env1,
        Compose(
            CatTensors(env1_obs_keys, "observation_stand", del_keys=False),
            CatTensors(env1_obs_keys, "observation"),
            DoubleToFloat(
                in_keys=["observation_stand", "observation"],
                in_keys_inv=["action"],
            ),
        ),
    )
    env2 = DMControlEnv("humanoid", "walk")
    env2_obs_keys = list(env2.observation_spec.keys())
    env2 = TransformedEnv(
        env2,
        Compose(
            CatTensors(env2_obs_keys, "observation_walk", del_keys=False),
            CatTensors(env2_obs_keys, "observation"),
            DoubleToFloat(
                in_keys=["observation_walk", "observation"],
                in_keys_inv=["action"],
            ),
        ),
    )








.. GENERATED FROM PYTHON SOURCE LINES 76-85

.. code-block:: Python


    tdreset1 = env1.reset()
    tdreset2 = env2.reset()

    # In TorchRL, stacking is done in a lazy manner: the original tensordicts
    # can still be recovered by indexing the main tensordict
    tdreset = torch.stack([tdreset1, tdreset2], 0)
    assert tdreset[0] is tdreset1








.. GENERATED FROM PYTHON SOURCE LINES 86-89

.. code-block:: Python


    print(tdreset[0])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            observation: Tensor(shape=torch.Size([67]), device=cpu, dtype=torch.float32, is_shared=False),
            observation_stand: Tensor(shape=torch.Size([67]), device=cpu, dtype=torch.float32, is_shared=False),
            terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 90-97

Policy
^^^^^^

We will design a policy where a backbone reads the "observation" key.
Then specific sub-components will ready the "observation_stand" and
"observation_walk" keys of the stacked tensordicts, if they are present,
and pass them through the dedicated sub-network.

.. GENERATED FROM PYTHON SOURCE LINES 97-100

.. code-block:: Python


    action_dim = env1.action_spec.shape[-1]








.. GENERATED FROM PYTHON SOURCE LINES 101-119

.. code-block:: Python


    policy_common = TensorDictModule(
        nn.Linear(67, 64), in_keys=["observation"], out_keys=["hidden"]
    )
    policy_stand = TensorDictModule(
        MLP(67 + 64, action_dim, depth=2),
        in_keys=["observation_stand", "hidden"],
        out_keys=["action"],
    )
    policy_walk = TensorDictModule(
        MLP(67 + 64, action_dim, depth=2),
        in_keys=["observation_walk", "hidden"],
        out_keys=["action"],
    )
    seq = TensorDictSequential(
        policy_common, policy_stand, policy_walk, partial_tolerant=True
    )








.. GENERATED FROM PYTHON SOURCE LINES 120-121

Let's check that our sequence outputs actions for a single env (stand).

.. GENERATED FROM PYTHON SOURCE LINES 121-124

.. code-block:: Python


    seq(env1.reset())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(shape=torch.Size([21]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            hidden: Tensor(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),
            observation: Tensor(shape=torch.Size([67]), device=cpu, dtype=torch.float32, is_shared=False),
            observation_stand: Tensor(shape=torch.Size([67]), device=cpu, dtype=torch.float32, is_shared=False),
            terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 125-126

Let's check that our sequence outputs actions for a single env (walk).

.. GENERATED FROM PYTHON SOURCE LINES 126-129

.. code-block:: Python


    seq(env2.reset())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(shape=torch.Size([21]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            hidden: Tensor(shape=torch.Size([64]), device=cpu, dtype=torch.float32, is_shared=False),
            observation: Tensor(shape=torch.Size([67]), device=cpu, dtype=torch.float32, is_shared=False),
            observation_walk: Tensor(shape=torch.Size([67]), device=cpu, dtype=torch.float32, is_shared=False),
            terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 130-134

This also works with the stack: now the stand and walk keys have
disappeared, because they're not shared by all tensordicts. But the
``TensorDictSequential`` still performed the operations. Note that the
backbone was executed in a vectorized way - not in a loop - which is more efficient.

.. GENERATED FROM PYTHON SOURCE LINES 134-137

.. code-block:: Python


    seq(tdreset)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    LazyStackedTensorDict(
        fields={
            action: Tensor(shape=torch.Size([2, 21]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([2, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            hidden: Tensor(shape=torch.Size([2, 64]), device=cpu, dtype=torch.float32, is_shared=False),
            observation: Tensor(shape=torch.Size([2, 67]), device=cpu, dtype=torch.float32, is_shared=False),
            terminated: Tensor(shape=torch.Size([2, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            truncated: Tensor(shape=torch.Size([2, 1]), device=cpu, dtype=torch.bool, is_shared=False)},
        exclusive_fields={
            0 ->
                observation_stand: Tensor(shape=torch.Size([67]), device=cpu, dtype=torch.float32, is_shared=False),
            1 ->
                observation_walk: Tensor(shape=torch.Size([67]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([2]),
        device=cpu,
        is_shared=False,
        stack_dim=0)



.. GENERATED FROM PYTHON SOURCE LINES 138-149

Executing diverse tasks in parallel
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We can parallelize the operations if the common keys-value pairs share the
same specs (in particular their shape and dtype must match: you can't do the
following if the observation shapes are different but are pointed to by the
same key).

If ParallelEnv receives a single env making function, it will assume that
a single task has to be performed. If a list of functions is provided, then
it will assume that we are in a multi-task setting.

.. GENERATED FROM PYTHON SOURCE LINES 149-187

.. code-block:: Python



    def env1_maker():
        return TransformedEnv(
            DMControlEnv("humanoid", "stand"),
            Compose(
                CatTensors(env1_obs_keys, "observation_stand", del_keys=False),
                CatTensors(env1_obs_keys, "observation"),
                DoubleToFloat(
                    in_keys=["observation_stand", "observation"],
                    in_keys_inv=["action"],
                ),
            ),
        )


    def env2_maker():
        return TransformedEnv(
            DMControlEnv("humanoid", "walk"),
            Compose(
                CatTensors(env2_obs_keys, "observation_walk", del_keys=False),
                CatTensors(env2_obs_keys, "observation"),
                DoubleToFloat(
                    in_keys=["observation_walk", "observation"],
                    in_keys_inv=["action"],
                ),
            ),
        )


    env = ParallelEnv(2, [env1_maker, env2_maker])
    assert not env._single_task

    tdreset = env.reset()
    print(tdreset)
    print(tdreset[0])
    print(tdreset[1])  # should be different



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/pytorch/rl/docs/source/reference/generated/tutorials/multi_task.py", line 182, in <module>
        tdreset = env.reset()
      File "/pytorch/rl/torchrl/envs/common.py", line 2069, in reset
        tensordict_reset = self._reset(tensordict, **kwargs)
      File "/pytorch/rl/env/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
        return func(*args, **kwargs)
      File "/pytorch/rl/torchrl/envs/batched_envs.py", line 56, in decorated_fun
        self._start_workers()
      File "/pytorch/rl/torchrl/envs/batched_envs.py", line 1135, in _start_workers
        process.start()
      File "/pytorch/rl/env/lib/python3.8/multiprocessing/process.py", line 121, in start
        self._popen = self._Popen(self)
      File "/pytorch/rl/env/lib/python3.8/multiprocessing/context.py", line 224, in _Popen
        return _default_context.get_context().Process._Popen(process_obj)
      File "/pytorch/rl/env/lib/python3.8/multiprocessing/context.py", line 284, in _Popen
        return Popen(process_obj)
      File "/pytorch/rl/env/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 32, in __init__
        super().__init__(process_obj)
      File "/pytorch/rl/env/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
        self._launch(process_obj)
      File "/pytorch/rl/env/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 47, in _launch
        reduction.dump(process_obj, fp)
      File "/pytorch/rl/env/lib/python3.8/multiprocessing/reduction.py", line 60, in dump
        ForkingPickler(file, protocol).dump(obj)
      File "/pytorch/rl/env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 295, in reduce_tensor
        from torch.nested._internal.nested_tensor import NestedTensor
      File "/pytorch/rl/env/lib/python3.8/site-packages/torch/nested/_internal/nested_tensor.py", line 6, in <module>
        from torch.fx.experimental.symbolic_shapes import has_free_symbols
      File "/pytorch/rl/env/lib/python3.8/site-packages/torch/fx/experimental/symbolic_shapes.py", line 63, in <module>
        from torch.utils._sympy.functions import FloorDiv, Mod, IsNonOverlappingAndDenseIndicator
      File "/pytorch/rl/env/lib/python3.8/site-packages/torch/utils/_sympy/functions.py", line 1, in <module>
        import sympy
      File "/pytorch/rl/env/lib/python3.8/site-packages/sympy/__init__.py", line 52, in <module>
        from .core import (sympify, SympifyError, cacheit, Basic, Atom,
      File "/pytorch/rl/env/lib/python3.8/site-packages/sympy/core/__init__.py", line 9, in <module>
        from .expr import Expr, AtomicExpr, UnevaluatedExpr
      File "/pytorch/rl/env/lib/python3.8/site-packages/sympy/core/expr.py", line 4159, in <module>
        from .mul import Mul
      File "/pytorch/rl/env/lib/python3.8/site-packages/sympy/core/mul.py", line 2193, in <module>
        from .numbers import Rational
      File "/pytorch/rl/env/lib/python3.8/site-packages/sympy/core/numbers.py", line 4567, in <module>
        _sympy_converter[type(mpmath.rational.mpq(1, 2))] = sympify_mpmath_mpq
    AttributeError: module 'mpmath' has no attribute 'rational'




.. GENERATED FROM PYTHON SOURCE LINES 188-189

Let's pass the output through our network.

.. GENERATED FROM PYTHON SOURCE LINES 189-201

.. code-block:: Python


    tdreset = seq(tdreset)
    print(tdreset)
    print(tdreset[0])
    print(tdreset[1])  # should be different but all have an "action" key


    env.step(tdreset)  # computes actions and execute steps in parallel
    print(tdreset)
    print(tdreset[0])
    print(tdreset[1])  # next_observation has now been written


.. GENERATED FROM PYTHON SOURCE LINES 202-204

Rollout
^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 204-207

.. code-block:: Python


    td_rollout = env.rollout(100, policy=seq, return_contiguous=False)


.. GENERATED FROM PYTHON SOURCE LINES 208-211

.. code-block:: Python


    td_rollout[:, 0]  # tensordict of the first step: only the common keys are shown


.. GENERATED FROM PYTHON SOURCE LINES 212-217

.. code-block:: Python


    td_rollout[0]  # tensordict of the first env: the stand obs is present

    env.close()
    del env


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 31.178 seconds)

**Estimated memory usage:**  564 MB


.. _sphx_glr_download_tutorials_multi_task.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: multi_task.ipynb <multi_task.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: multi_task.py <multi_task.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
